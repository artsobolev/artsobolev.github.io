<!DOCTYPE html>
<html lang="en">
<head>
  <title>Articles about stochastic computation graphs series – B.log</title>
  <meta charset="utf-8" />

  <meta property="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta property="og:title" content="Articles about stochastic computation graphs series – B.log" />
  <meta property="og:description" content="Articles about stochastic computation graphs series" />

  <link rel="shortcut icon" href="/favicon.ico"/>

  <link rel="stylesheet" type="text/css" href="/theme/css/default.css" />
  <link rel="stylesheet" type="text/css" href="/theme/css/syntax.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:,b" />
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        macros: {
	  E: '\\mathop{\\mathbb{E}}'
	}
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="/">B.log</a></h1>
        <h2>Random notes mostly on Machine Learning</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="/">Home</a>
          <a href="/pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
<h2 class="tags-pane">Articles tagged <a href="/tags/stochastic-computation-graphs-series.html">stochastic computation graphs series</a></h2>

    <article>
        <header>
            <h3><a href="/posts/2017-11-12-stochastic-computation-graphs-fixing-reinforce.html">Stochastic Computation Graphs: Fixing REINFORCE</a></h3>
            <time>November 12, 2017</time>
        </header>

        <section><p>This is the final post of the <a href="/tags/stochastic-computation-graphs-series.html">stochastic computation graphs series</a>. Last time we discussed models with <a href="/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html">discrete relaxations of stochastic nodes</a>, which allowed us to employ the power of reparametrization.</p>
<p>These methods, however, posses one flaw: they consider different models, thus introducing inherent bias – your test time discrete model will be doing something different from what your training time model did. Therefore in this post we'll get back to the REINFORCE aka Score Function estimator, and see if we can fix its problems.</p>
</section>
    </article>

    <hr/>

    <article>
        <header>
            <h3><a href="/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html">Stochastic Computation Graphs: Discrete Relaxations</a></h3>
            <time>October 28, 2017</time>
        </header>

        <section><p>This is the second post of the <a href="/tags/stochastic-computation-graphs-series.html">stochastic computation graphs series</a>. Last time we discussed models with <a href="/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html">continuous stochastic nodes</a>, for which there are powerful reparametrization technics.</p>
<p>Unfortunately, these methods don't work for discrete random variables. Moreover, it looks like there's no way to backpropagate through discrete stochastic nodes, as there's no infinitesimal change of random values when you infinitesimally perturb their parameters.</p>
<p>In this post I'll talk about continuous relaxations of discrete random variables.</p>
</section>
    </article>

    <hr/>

    <article>
        <header>
            <h3><a href="/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html">Stochastic Computation Graphs: Continuous Case</a></h3>
            <time>September 10, 2017</time>
        </header>

        <section><p>Last year I covered <a href="/tags/modern-variational-inference-series.html">some modern Variational Inference theory</a>. These methods are often used in conjunction with Deep Neural Networks to form deep generative models (VAE, for example) or to enrich deterministic models with stochastic control, which leads to better exploration. Or you might be interested in amortized inference.</p>
<p>All these cases turn your computation graph into a stochastic one – previously deterministic nodes now become random. And it's not obvious how to do backpropagation through these nodes. In <a href="/tags/stochastic-computation-graphs-series.html">this series</a> I'd like to outline possible approaches. This time we're going to see why general approach works poorly, and see what we can do in a continuous case.</p>
</section>
    </article>

    <hr/>

<!-- /#posts-list -->

    </section>
    <footer>
        Generated with Pelican 
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>