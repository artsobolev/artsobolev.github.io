<!DOCTYPE html>
<html lang="en">
<head>
  <title>Neural Variational Inference: Importance Weighted Autoencoders – B.log</title>
  <meta charset="utf-8" />

  <meta property="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta property="og:title" content="Neural Variational Inference: Importance Weighted Autoencoders – B.log" />
  <meta property="og:description" content="Previously we covered Variational Autoencoders (VAE) — popular inference tool based on neural networks. In this post we'll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, Importance Weighted Autoencoders (IWAE)...." />

  <link rel="shortcut icon" href="/favicon.ico"/>

  <link rel="stylesheet" type="text/css" href="/theme/css/default.css" />
  <link rel="stylesheet" type="text/css" href="/theme/css/syntax.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:,b" />
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        macros: {
	  E: '\\mathop{\\mathbb{E}}'
	}
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="/">B.log</a></h1>
        <h2>Random notes mostly on Machine Learning</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="/">Home</a>
          <a href="/pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
<article>
<header>
	<h1>Neural Variational Inference: Importance Weighted Autoencoders</h1>
	<time>July 14, 2016</time>
</header>

<section class="post-body">
    <p>Previously we covered <a href="/posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">Variational Autoencoders</a> (VAE) — popular inference tool based on neural networks. In this post we'll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, <a href="https://arxiv.org/abs/1509.00519">Importance Weighted Autoencoders</a> (IWAE). The crucial contribution of this work is introduction of a new lower-bound on the marginal log-likelihood $\log p(x)$ which generalizes ELBO, but also allows one to use less accurate approximate posteriors $q(z \mid x, \Lambda)$.</p>
<p>On a dessert we'll discuss another paper, <a href="https://arxiv.org/abs/1602.06725">Variational inference for Monte Carlo objectives</a> by A. Mnih and D. Rezende which aims to broaden the applicability of this approach to models where reparametrization trick can not be used (e.g. for discrete variables).</p>
<!--more-->

<h3>Importance Weighted Autoencoders</h3>

<p>Let's first answer the question of how one can come up with a lower bound for the marginal log-likelihood? In the very beginning of the series, <a href="/posts/2016-07-01-neural-variational-inference-classical-theory.html">Classical Theory</a> post, we used some trickery to come up with the ELBO. That massaging of the marginal log-likelihood wasn't particulary enlightening on how one could invent that lower bound. Now we're going to consider a principled approach to invention of new lower bounds based on <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen's inequality</a>.</p>
<p>Suppose we have some unbiased estimator $f(x, z)$ of $p(x)$, that is, $\E_{q(z)} f(x, z) = p(x)$. Then</p>
<p>$$
\log p(x) = \log \E_{q(z)} f(x, z) \stackrel{\text{Jensen}}{\ge} \E_{q(z)} \log f(x, z)
$$</p>
<p>In particular, if $z \sim q(z \mid x)$ and $f(x, z) = \tfrac{p(x, z)}{q(z \mid x)}$, we obtain the standard ELBO. The IWAE paper proposes another estimate (actually, a family of estimators parametrized by an integer $K$) of the marginal $p(x)$:</p>
<p>$$
f(x, z_1, \dots, z_K) = \frac{1}{K} \sum_{k=1}^K \frac{p(x, z_k)}{q(z_k \mid x)}
$$</p>
<p>Where each $z_k$ comes from the same distribution $q(z_k \mid x) = q(z \mid x)$.
Obviously, $f(x, z_1, \dots, z_K)$ is still an unbiased estimator of the $p(x)$, and therefore $\E_{q(z_{1 \dots K}|x)} \log f(x, z_1, \dots, z_K)$ is a valid lower-bound on the marginal log-likelihood.</p>
<p>Let's analyze this new lower-bound now. First, let's dissect the ELBO:</p>
<p>$$
\mathcal{L}(\Theta, \Lambda)
= \E_{q(z_1|x) \dots q(z_K|x)} \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)}
= \E_{q(z_1|x) \dots q(z_K|x)} \left[\log \frac{p(z \mid x, \Theta)}{q(z \mid x, \Lambda)} \right] + \log p(x \mid \Theta)
$$</p>
<p>If $q$ approximates the true posterior accurately, the first term (which is a KL-divergence, BTW) is close to zero. However, when estmating it using Monte Carlo samples, the ELBO heavily penalizes inaccurate approximations: if $q(z \mid x, \Lambda)$ gives us samples from high probability regions of the true posterior $p(z \mid x, \Theta)$ only occasionally (like 20% of times), the gap between the ELBO and the marginal log-likelihood would be huge ($p(z\mid x, \Theta)$ is small, $q(z \mid x, \Lambda)$ is big), which does not help learning. As you might have guessed, IWAE allows us to use several samples. Let's see it in detail:</p>
<p>$$
\begin{align*}
\mathcal{L}_K(\Theta, \Lambda)
&amp;= \E_{q(z_1|x) \dots q(z_K|x)} \left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(x, z_k \mid \Theta)}{q(z_k \mid x, \Lambda)} \right] \\
&amp;= \E_{q(z_1|x) \dots q(z_K|x)} \left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(z_k \mid x, \Theta)}{q(z_k \mid x, \Lambda)} \right] + \log p(x \mid \Theta)
\end{align*}
$$</p>
<p>This averaging of posterior ratios saves us from bad samples screwing the lower bound, as it'll be pushed up by good samples (provided the approximation has a reasonable probability of generating a good sample in $K$ attempts). This allows one to perform model inference even with poor approximations $q(z \mid x, \Lambda)$. The more samples $K$ we use — the less accurate approximation we can tolerate. In fact, authors prove the following theorem:</p>
<blockquote>
<strong>Theorem 1</strong>. For all $K$, the lower bounds satisfy
$$
\log p(x \mid \Theta) \ge \mathcal{L}_{K+1}(\Theta, \Lambda) \ge \mathcal{L}_{K}(\Theta, \Lambda)
$$

Moreover, if $p(z, x \mid \Theta) / q(z \mid x, \Lambda)$ is bounded, then $\mathcal{L}_{K}(\Theta, \Lambda)$ approaches $\log p(x \mid \Theta)$ as $K$ goes to infinity.
</blockquote>

<p>The convergence result follows from the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law">strong law of large numbers</a>.</p>
<p>As with VAE, we use the reparametrization trick to avoid backpropagation through stochastic units:</p>
<p>$$
\mathcal{L}_K(\Theta, \Lambda) = \E_{\varepsilon_1, \dots, \varepsilon_K} \log \frac{1}{K} \sum_{k=1}^K \overbrace{\frac{p(x, g(\varepsilon_k; \Lambda) \mid \Theta)}{q(g(\varepsilon_k; \Lambda) \mid x, \Lambda)}}^{w(x, \varepsilon_k, \Theta, \Lambda)}
$$</p>
<p>The gradients then are</p>
<p>$$
\nabla_\Theta \mathcal{L}_K(\Theta, \Lambda) = \E_{\varepsilon_1, \dots, \varepsilon_K} \sum_{k=1}^K \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) \nabla_\Theta \log w(x, \varepsilon_k, \Theta, \Lambda)
$$
$$
\nabla_\Lambda \mathcal{L}_K(\Theta, \Lambda) = \E_{\varepsilon_1, \dots, \varepsilon_K} \sum_{k=1}^K \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) \nabla_\Lambda \log w(x, \varepsilon_k, \Theta, \Lambda)
$$
$$
\text{where } \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) := \frac{w(x, \varepsilon_k, \Theta, \Lambda)}{\sum_{k=1}^K w(x, \varepsilon_k, \Theta, \Lambda)}
$$</p>
<p>(We used the identity $\nabla_x f(x) = f(x) \nabla_x \log f(x)$ here).</p>
<p>Just as one would expect, setting $K=1$ reduces these gradients to ones we've seen in VAEs as the only importance weight $\hat w_1$ is equal to 1. Unfortunatelly, this approach does not allow one to decompose the lower-bound into the reconstruction error and KL-divergence to analytically compute the later. However, authors report indistinguishable performance of 2 approaches (with KL computed analytically or estimated using Monte Carlo) in case of $K=1$.</p>
<p>BTW, <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html">Hugo Larochelle</a> writes <a href="https://twitter.com/hugo_larochelle/timelines/639067398511968256">notes</a> on different papers, and he has written and made publicly available <a href="https://www.evernote.com/shard/s189/sh/e2c8a133-1814-474c-b267-366600a1921b/06a756f1618cd47ababc7aae0e514dbf">Notes on Importance Weighted Autoencoders</a>.</p>
<h3>Variational inference for Monte Carlo objectives</h3>

<p>As I said in the introduction, IWAE has been "generalized" to discrete variables — a case when one can not employ the reparametrization trick, and instead has to somehow reduce high variance of a score function-based estimator. Previously, during our discussion of the <a href="/posts/2016-07-05-neural-variational-inference-blackbox.html">Blackbox VI and variance reduction techniques</a> we covered NVIL (Neural Variational Inference and Learning) estimator, which uses another neural network to estimate marginal likelihood and reduce the variance. This work is built upon a similar idea.</p>
<p>First, let's derive score-function-based gradients for variational parameters $\Lambda$ (where $w$ now is defined as $w(x, z, \Theta, \Lambda) = \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)}$, and $\hat w$ is a normalized across all samples $z_{1\dots K}$ version):</p>
<p>$$
\begin{align}
\nabla_\Lambda \mathcal{L}_K(\Theta, \Lambda)
&amp;= \nabla_\Lambda \E_{q(z_1, \dots, z_K \mid x, \Lambda)} \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \\
&amp;= \nabla_\Lambda \int q(z_1, \dots, z_K \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \; dz_1 \dots dz_K \\
&amp;= \E_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \nabla_\Lambda \log q(z_k \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \right] \\
&amp; \quad+ \E_{q(z_1, \dots, z_K \mid x, \Lambda)} \nabla_\Lambda \log \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \\
&amp;= \E_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \nabla_\Lambda \log q(z_k \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \right] \\
&amp; \quad+ \E_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \hat w_k(x, z_{1 \dots K}, \Theta, \Lambda) \nabla_\Lambda \log w(x, z_k, \Theta, \Lambda) \right]
\end{align}
$$</p>
<p>The second term is exactly the gradient of the reparametrized case, and it does not cause us any troubles. The first term, however has some issues.</p>
<p>First, it does not distinguish individual samples' contributions: indeed, gradients for all samples have the same weight of $\log \tfrac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda)$ (called the <em>learning signal</em>) regardless of how probable they're in terms of the true posterior (that is, how well they describe an observation $x$). Compare it with the second term, where gradient for each sample $z_k$ is weighted in proportion to its importance weight $\hat w_k$.</p>
<p>Second problem is that the learning signal is unbounded, and can be quite high. Again, the second term does not suffer this as importance weights $\hat w_k$ are normalized to sum to 1.</p>
<p>One can use the NVIL estimator we've discussed previously to reduce the variance due to large magnitude of a learning signal. However, it does not address the problem of all gradients having the same weight. For this the authors propose to introduce per-sample baselines that minimize dependencies between samples.</p>
<p>This paper has also caught Dr. Larochelle's attention: <a href="https://www.evernote.com/shard/s189/sh/54a9fb88-1a71-4e8a-b0e3-f13480a68b8d/0663de49b93d397f519c7d7f73b6a441">Notes on Variational inference for Monte Carlo objectives</a>.</p>
</section>

<div class="tags-pane">Tagged as <a href="/tags/machine-learning.html">machine learning</a>, <a href="/tags/deep-learning.html">deep learning</a>, <a href="/tags/variational-inference.html">variational inference</a>, <a href="/tags/modern-variational-inference-series.html">modern variational inference series</a></div>
</article>

<section class="post-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'barmaley-exe'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
    </section>
    <footer>
        Generated with Pelican 
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>