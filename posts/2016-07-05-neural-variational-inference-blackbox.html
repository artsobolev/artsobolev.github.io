<!DOCTYPE html>
<html lang="en">
<head>
  <title>Neural Variational Inference: Blackbox Mode – Sobolev.space</title>
  <meta charset="utf-8" />

  <meta property="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta property="og:title" content="Neural Variational Inference: Blackbox Mode – Sobolev.space" />
  <meta property="og:description" content="In the previous post we covered Stochastic VI: an efficient and scalable variational inference method for exponential family models. However, there're many more distributions than those belonging to the exponential family. Inference in these cases..." />

  <link rel="shortcut icon" href="/favicon.ico"/>

  <link rel="stylesheet" type="text/css" href="/theme/css/default.css" />
  <link rel="stylesheet" type="text/css" href="/theme/css/syntax.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:,b" />
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        macros: {
	  E: '\\mathop{\\mathbb{E}}'
	}
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="/">Sobolev.space</a></h1>
        <h2>Random notes mostly on Machine Learning</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="/">Home</a>
          <a href="/pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
<article>
<header>
	<h1>Neural Variational Inference: Blackbox Mode</h1>
	<time>July 5, 2016</time>
</header>

<section class="post-body">
    <p>In the <a href="/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html">previous post</a> we covered Stochastic VI: an efficient and scalable variational inference method for exponential family models. However, there're many more distributions than those belonging to the exponential family. Inference in these cases requires significant amount of model analysis. In this post we consider <a href="https://arxiv.org/abs/1401.0118">Black Box Variational Inference</a> by Ranganath et al. This work just as the previous one comes from <a href="http://www.cs.columbia.edu/~blei/">David Blei</a> lab — one of the leading researchers in VI. And, just for the dessert, we'll touch upon another paper, which will finally introduce some neural networks in VI.</p>
<!--more-->

<h3>Blackbox Variational Inference</h3>

<p>As we have learned so far, the goal of VI is to maximize the ELBO $\mathcal{L}(\Theta, \Lambda)$. When we maximize it by $\Lambda$, we decrease the gap between the marginal likelihood of the model considered $\log p(x \mid \Theta)$, and when we maximize it by $\Theta$ we acltually fit the model. So let's concentrate on optimizing this objective:</p>
<p>$$
\mathcal{L}(\Theta, \Lambda) = \mathbb{E}_{q(z \mid x, \Lambda)} \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right]
$$</p>
<p>Let's find gradients of this objective:</p>
<p>$$
\begin{align}
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
&amp;= \nabla_{\Lambda} \int q(z \mid x, \Lambda) \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right] dz  \\
&amp;= \int \nabla_{\Lambda} q(z \mid x, \Lambda) \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right] dz - \int q(z \mid x, \Lambda) \nabla_{\Lambda} \log q(z \mid x, \Lambda) dz  \\
&amp;= \mathbb{E}_{q} \left[\frac{\nabla_{\Lambda} q(z \mid x, \Lambda)}{q(z \mid x, \Lambda)} \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \int q(z \mid x, \Lambda) \frac{\nabla_{\Lambda} q(z \mid x, \Lambda)}{q(z \mid x, \Lambda)} dz \\
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \int \nabla_{\Lambda} q(z \mid x, \Lambda) dz \\
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \nabla_{\Lambda} \overbrace{\int q(z \mid x, \Lambda) dz}^{=1} \\
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right]
\end{align}
$$</p>
<p>In statistics $\nabla_\Lambda \log q(z \mid x, \Lambda)$ is known as <a href="https://en.wikipedia.org/wiki/Score_(statistics)">score function</a>. For more on this "trick" see <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">a blogpost by Shakir Mohamed</a>. In many cases of practical interest $\log p(x, z, \mid \Theta)$ is too complicated to compute this expectation in closed form. Recall that we already used stochastic optimization successfully, so we can settle with just an estimate of true gradient. We get one by approximating the expectation using Monte-Carlo estimates using $L$ samples $z^{(l)} \sim q(z \mid x, \Lambda)$ (in practice we sometimes use just $L=1$ sample. We expect correct averaging to happen automagically due to use of minibatches):</p>
<p>$$
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)}
$$</p>
<p>For model parameters $\Theta$ gradients look even simpler, as we don't need to differentiate w.r.t. expectation distribution's parameters:</p>
<p>$$
\begin{align}
\nabla_{\Theta} \mathcal{L}(\Theta, \Lambda)
&amp;= \mathbb{E}_{q} \nabla_{\Theta} \log p(x, z \mid \Theta)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Theta} \log p(x, z^{(l)} \mid \Theta)
\end{align}
$$</p>
<p>We can even "naturalize" these gradients by premultiplying by the inverse Fisher Information Matrix $\mathcal{I}(\Lambda)^{-1}$.
And that's it! Much simpler than before, right? Of course, there's no free lunch, so there must be a catch... And there is: performance of stochastic optimization methods crucially depends on the variance of gradient estimators. It makes perfect sense: the higher the variance — the less information about the step direction we get. And unfortunately, in practice the aforementioned estimator based on the score function has impractically high variance. Luckily, in Monte Carlo community there are many variance reductions techniques known, we now describe some of them.</p>
<p>The first technique we'll describe is <strong>Rao-Blackwellization</strong>. The idea is simple: if it's possible to compute the expectation w.r.t. some of random variables, you should do it. If you think of it, it's an obvious advice as you essentially reduce amount of randomness in your Monte Carlo estimates. But let's put it more formally: we use chain rule to rewrite joint expectation as marginal expectation of conditional one:</p>
<p>$$
\mathbb{E}_{X, Y} f(X, Y) = \mathbb{E}_X \left[ \mathbb{E}_{Y \mid X} f(X, Y) \right]
$$</p>
<p>Let's see what happens with variance (in scalar case) when we estimate expectation of $\mathbb{E}_{Y \mid X} f(X, Y)$ instead of expectation of $f(X, Y)$:</p>
<p>$$
\begin{align}
\text{Var}_X(\mathbb{E}_{Y \mid X} f(X, Y))
&amp;= \mathbb{E} (\mathbb{E}_{Y \mid X} f(X, Y))^2 - (\mathbb{E}_{X, Y} f(X, Y))^2 \\
&amp;= \text{Var}_{X,Y}(f(X, Y)) - \mathbb{E}_X \left(\mathbb{E}_{Y \mid X} f(X, Y)^2 - (\mathbb{E}_{Y \mid X} f(X, Y))^2  \right) \\
&amp;= \text{Var}_{X,Y}(f(X, Y)) - \mathbb{E}_X \text{Var}_{Y\mid X} (f(X, Y))
\end{align}
$$</p>
<p>This formula says that Rao-Blackwellizing an estimator reduces its variance by $\mathbb{E}_X \text{Var}_{Y\mid X} (f(X, Y))$. Indeed, you can think of this term as of a measure of how much information $Y$ contains about $X$ that's relevant to computing $f(X, Y)$. Suppose $Y = X$: then you have $\mathbb{E}_X f(X, X)$, and taking expectation w.r.t. $Y$ does not reduce amount of randomness in the estimator. And this is what the formula tells us as $\text{Var}_{Y \mid X} f(X, Y)$ would be 0 in this case. Here's another example: suppose $f$ does not use $X$ at all: then only randomness in $Y$ affects the estimate, and after Rao-Blackwellization we expect the variance to drop to 0. And the formula agrees with out expectations as $\mathbb{E}_X \text{Var}_{Y \mid X} f(X, Y) = \text{Var}_Y f(X, Y)$ for any $X$ since $f(X, Y)$ does not depend on $X$.</p>
<p>Next technique is <strong>Control Variates</strong>, which is slightly less intuitive. The idea is that we can add zero-mean function $h(X)$ that'll preserve the expectation, but reduce the variance. Again, for a scalar case</p>
<p>$$
\text{Var}(f(X) - \alpha h(X)) = \text{Var}(f(X)) - 2 \alpha \text{Cov}(f(X), h(X)) + \alpha^2 \text{Var}(f(X))
$$</p>
<p>Optimal $\alpha^* = \frac{\text{Cov}(f(X), h(X))}{\text{Var}(f(X))}$. This formula reflects an obvious fact: if we want to reduce the variance, $h(X)$ must be correlated with $f(X)$. Sign of correlation does not matter, as $\alpha^*$ will adjust. BTW, in reinforcement learning $\alpha$ is called <strong>baseline</strong>.</p>
<p>As we already have learned, $\mathbb{E}_{q(z \mid x, \Lambda)} \nabla_\Lambda \log q(z \mid x, \Lambda) = 0$, so the score function is a good candidate for $h(x)$. Therefore our estimates become</p>
<p>$$
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \circ \left(\log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* \right)
$$</p>
<p>Where $\circ$ is pointwise multiplication and $\alpha$ is a vector of $|\Lambda|$ components with $\alpha_i$ being a baseline for variational parameter $\Lambda_i$:</p>
<p>$$
\alpha^*_i = \frac{\text{Cov}(\nabla_{\Lambda_i} \log q(z \mid x, \Lambda)\left( \log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right), \nabla_{\Lambda_i} \log q(z \mid x, \Lambda))}{\text{Var}(\nabla_{\Lambda_i} \log q(z \mid x, \Lambda)\left( \log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right))}
$$</p>
<h3>Neural Variational Inference and Learning</h3>

<p>Hoooray, neural networks! In this section I'll briefly describe a variance reduction technique coined by A. Mnih and K. Gregor in <a href="https://arxiv.org/abs/1402.0030">Neural Variational Inference and Learning in Belief Networks</a>. The idea is surprisingly simple: why not learn a baseline $\alpha$ using a neural network?</p>
<p>$$
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \circ \left(\log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* - \alpha(x) \right)
$$</p>
<p>Where $\alpha(x)$ is a neural network trained to minimize</p>
<p>$$
 \mathbb{E}_{q(z \mid x, \Lambda)} \left( \log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* - \alpha(x) \right)^2
$$</p>
<p>What's the motivation of this objective? The gradient step of $\nabla_\Lambda \mathcal{L}(\Theta, \Lambda)$ can be seen as pushing $q(z\mid x, \Lambda)$ towards $p(x, z \mid \Theta)$. Since $q$ has to be normalized like any other proper distribution, it's actually pushed towards the true posterior $p(z \mid x, \Theta)$. We can rewrite the gradient $\nabla_\Lambda \mathcal{L}(\Theta, \Lambda)$ as</p>
<p>$$
\begin{align}
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \left(\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right) \right] \\
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \left(\log p(z \mid x, \Theta) - \log q(z \mid x, \Lambda) + \log p(x \mid \Theta) \right) \right]
\end{align}
$$</p>
<p>While this additional $\log p(x \mid \Theta)$ term does not contribute to the expectation, it affects the variance on the estimator. Therefore, $\alpha(x)$ is supposed to estimate the marginal log-likelihood $\log p(x \mid \Theta)$.</p>
<p>The paper also lists several other variance reduction techniques that can be used in combination with the neural network-based baseline:</p>
<ul>
<li><strong>Constant baseline</strong> — analogue of <em>Control Variates</em>, uses running average of $\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda)$ as a baseline</li>
<li><strong>Variance normalization</strong> — normalizes the learning signal to unit variance, equivalent to adaptive learning rate</li>
<li><strong>Local learning signals</strong> — falls out of the scope of this post as requires it model-specific analysis and alternations, and can't be used in Blackbox regime</li>
</ul>
</section>

<div class="tags-pane">Tagged as <a href="/tags/machine-learning.html">machine learning</a>, <a href="/tags/deep-learning.html">deep learning</a>, <a href="/tags/variational-inference.html">variational inference</a>, <a href="/tags/modern-variational-inference-series.html">modern variational inference series</a></div>
</article>

<section class="post-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'barmaley-exe'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
    </section>
    <footer>
        Generated with Pelican 
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>